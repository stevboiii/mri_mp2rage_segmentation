{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pyd4VeelGDv"
      },
      "source": [
        "# This is the validation and actual use program for 3D U-net for MP2RAGE MRI images\n",
        "Validation was done using 300 axial DICOM files from the same patient of size [512, 512]\n",
        "\n",
        "Once again, make sure to update and verify all of your file and folder directories\n",
        "\n",
        "Details about hardware requirements will be specified in the README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ0_rwXfC3bb"
      },
      "source": [
        "## Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F1q7A3mMR_lC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pydicom import dcmread\n",
        "import pydicom\n",
        "import nibabel as nib\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Nw1f6qjfLI"
      },
      "source": [
        "### If necessary: Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-MDHew3SadH",
        "outputId": "9c73e7b6-f58d-464b-e0af-556080a87aea"
      },
      "outputs": [],
      "source": [
        "#packages that need to be installed in google colab\n",
        "!pip install pydicom\n",
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4XRFK2tijhj"
      },
      "source": [
        "### If necessary: mount to drive\n",
        "Only with Google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgI-ViySTm8k",
        "outputId": "0b647294-c910-4941-8f9a-9ecf14869994"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gol8_GpUC-5I"
      },
      "source": [
        "## Creating dataset\n",
        "Validation dataset involves 300 axial images with size 512x512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "662_eCivTeic"
      },
      "outputs": [],
      "source": [
        "#dataset of nii.gz files\n",
        "class dicomDataset(Dataset):\n",
        "    def __init__(self, inv1_dir, inv2_dir, t1map_dir, transform=None):\n",
        "        self.inv1_paths = sorted([os.path.join(inv1_dir, f) for f in os.listdir(inv1_dir)])\n",
        "        self.inv2_paths = sorted([os.path.join(inv2_dir, f) for f in os.listdir(inv2_dir)])\n",
        "        self.t1map_paths = sorted([os.path.join(t1map_dir, f) for f in os.listdir(t1map_dir)])\n",
        "        self.transform = transform   #none for now\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inv1_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inv1_info = dcmread(self.inv1_paths[idx])\n",
        "        inv1_img = inv1_info.pixel_array\n",
        "        # if 'PixelData' in inv1_info:\n",
        "            # del inv1_info.PixelData       #keeps all except image data, stored seperately to be not edited\n",
        "\n",
        "        inv2_info = dcmread(self.inv2_paths[idx])\n",
        "        inv2_img = inv2_info.pixel_array\n",
        "        # if 'PixelData' in inv2_info:\n",
        "            # del inv2_info.PixelData\n",
        "\n",
        "        t1map_info = dcmread(self.t1map_paths[idx])\n",
        "        t1map_img = t1map_info.pixel_array\n",
        "        # if 'PixelData' in t1map_info:\n",
        "            # del t1map_info.PixelData\n",
        "\n",
        "        inv1_img = np.expand_dims(inv1_img, axis=0)  # [1, D, H, W]\n",
        "        inv2_img = np.expand_dims(inv2_img, axis=0)  # [1, D, H, W]\n",
        "        t1map_img = np.expand_dims(t1map_img, axis=0)  # [1, D, H, W]\n",
        "\n",
        "        #convert to tensor\n",
        "        inv1_img = torch.tensor(inv1_img, dtype=torch.float32)\n",
        "        inv2_img = torch.tensor(inv2_img, dtype=torch.float32)\n",
        "        t1map_img = torch.tensor(t1map_img, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            inv1_img, inv2_img, t1map_img = self.transform(inv1_img, inv2_img, t1map_img)\n",
        "\n",
        "        return inv1_img, inv2_img, t1map_img, inv1_info, inv2_info, t1map_info\n",
        "\n",
        "inv1dir = './lemon_data/mp2rage_ex3750/inv1'\n",
        "inv2dir = './lemon_data/mp2rage_ex3750/inv2'\n",
        "t1mapdir = './lemon_data/mp2rage_ex3750/t1map'\n",
        "\n",
        "\n",
        "dataset = dicomDataset(inv1dir, inv2dir, t1mapdir)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggFC7KJBUZms"
      },
      "source": [
        "## Important functions\n",
        "stackDataset is for processing a dataset of dicom images into one 3d volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epq1CeT2UiKl"
      },
      "outputs": [],
      "source": [
        "def stackDataset(img_type, dataset=dataset):\n",
        "  # process into 3d\n",
        "  stackList = []\n",
        "  stack_images_index = 0\n",
        "  while stack_images_index < len(dataset):\n",
        "      nextImg = dataset[stack_images_index][img_type]\n",
        "      #nextImg = shrink_transform(nextImg)\n",
        "      stackList.append(nextImg)\n",
        "      stack_images_index += 1\n",
        "  stackImg = torch.stack(stackList, dim=0)\n",
        "  print(stackImg.size())\n",
        "  return stackImg # Return the stacked tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP27jThGZqdo"
      },
      "source": [
        "Function to normalize tensors to [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5sfRmKXoT4A7"
      },
      "outputs": [],
      "source": [
        "#normalize to [-1, 1]\n",
        "\n",
        "def normalize_to_neg_one(tensor):\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "\n",
        "    # Avoid divide-by-zero error if max == min\n",
        "    if max_val == min_val:\n",
        "        return torch.zeros_like(tensor)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    norm = (tensor - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Scale to [-1, 1]\n",
        "    norm = norm * 2 - 1\n",
        "    return norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhj6RJRfZkjp"
      },
      "source": [
        "Define 3D U-net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HVEHsgSwWR6A"
      },
      "outputs": [],
      "source": [
        "#convolution block for a u-net\n",
        "class ConvBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "#3D U-net class\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock3D(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.enc2 = ConvBlock3D(32, 64)\n",
        "        self.pool2 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.bottleneck = ConvBlock3D(64, 128)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
        "        self.dec2 = ConvBlock3D(128, 64)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
        "        self.dec1 = ConvBlock3D(64, 32)\n",
        "\n",
        "        self.final = nn.Conv3d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        b = self.bottleneck(self.pool2(e2))\n",
        "\n",
        "        d2 = self.upconv2(b)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        return self.final(d1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ8WP7s8dTOG"
      },
      "source": [
        "Set device to CUDA or CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1LS81VbX7c6"
      },
      "outputs": [],
      "source": [
        "#use cuda or cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_p0ork5dW6l"
      },
      "source": [
        "Predict: generate a mask based on an input\n",
        "\n",
        "maskEditing3D: Set all background values to -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EjUNea2bWAsG"
      },
      "outputs": [],
      "source": [
        "#function to predict mask using data\n",
        "def predict(model, image_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Reshape tensor to [Batch, Channels, Depth, Height, Width]\n",
        "        # Assuming image_tensor is [D, H, W] or [1, D, H, W]\n",
        "        if image_tensor.ndim == 3:\n",
        "            image_tensor = image_tensor.unsqueeze(0).unsqueeze(0) # [1, 1, D, H, W]\n",
        "        elif image_tensor.ndim == 4:\n",
        "            image_tensor = image_tensor.unsqueeze(0) # [1, 1, D, H, W]\n",
        "\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        output = model(image_tensor)\n",
        "        return output\n",
        "\n",
        "#edit an image based on mask\n",
        "def maskEditing3D(image, mask, threshold=-0.5):\n",
        "    new_img = torch.tensor(image.copy())  # Create a new tensor from a NumPy copy\n",
        "    if not torch.is_tensor(mask):\n",
        "        mask = torch.tensor(mask)\n",
        "\n",
        "    for sliceIdx in range(new_img.size(0)):\n",
        "        for l in range(new_img.size(1)):\n",
        "            for w in range(new_img.size(2)):\n",
        "                if mask[sliceIdx, l, w] < threshold:\n",
        "                    new_img[sliceIdx, l, w] = -1  #set background voxels to -1\n",
        "\n",
        "    return new_img  # return as numpy array to keep format consistent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yge8lnqdlD7"
      },
      "source": [
        "Load t1map and inv2 volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxH4IgK6aj4v"
      },
      "outputs": [],
      "source": [
        "#load t1map file\n",
        "t1map_3d = stackDataset(2).squeeze(1).squeeze(1).unsqueeze(0).squeeze(1)\n",
        "t1map_3d = F.interpolate(t1map_3d.unsqueeze(0).unsqueeze(0), size=(300, 256, 240), mode='trilinear').squeeze(0).squeeze(0)\n",
        "\n",
        "#load inv2 file\n",
        "inv2_3d = stackDataset(1).squeeze(1).squeeze(1).unsqueeze(0).squeeze(1)\n",
        "inv2_3d = F.interpolate(inv2_3d.unsqueeze(0).unsqueeze(0), size=(300, 256, 240), mode='trilinear').squeeze(0).squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzQTyGtQUGJd"
      },
      "source": [
        "## Working with inv2-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET9eR3XRbREI"
      },
      "source": [
        "import params from training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qlVtFkFWTBT",
        "outputId": "42b7bd4d-7b21-46bd-e0b1-463d8d1acf73"
      },
      "outputs": [],
      "source": [
        "#inv2 model\n",
        "inv2model = UNet3D().to(device)\n",
        "#load params\n",
        "inv2model.load_state_dict(torch.load('./3d_segmentation_inv2.pth', map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hivLlQwtbXf5"
      },
      "source": [
        "Create mask and edit t1map with mask, t1map needs to be normalized to be edited first, but cannot affect original file as training was done without normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55TKXvz0AXgt"
      },
      "outputs": [],
      "source": [
        "#generate inv2 mask\n",
        "inv2_mask3d = predict(inv2model, inv2_3d.unsqueeze(0).unsqueeze(0)).squeeze(1).squeeze(0)   #maintain size at [300, 256, 240]\n",
        "inv2_mask3d = normalize_to_neg_one(inv2_mask3d)\n",
        "\n",
        "normalized_t1map_3d = normalize_to_neg_one(t1map_3d)\n",
        "\n",
        "#edit t1map with inv2 mask, input size is [1, 1, C, H, W]\n",
        "edited_t1map_3d_from_inv2 = maskEditing3D(normalized_t1map_3d.numpy().copy(), inv2_mask3d, threshold=-0.4)\n",
        "\n",
        "#resize output and mask back to [8, 512, 512]\n",
        "edited_t1map_3d_from_inv2 = F.interpolate(edited_t1map_3d_from_inv2.unsqueeze(0).unsqueeze(0), size=(300, 512, 512), mode='trilinear').squeeze(0).squeeze(0)\n",
        "inv2_mask3d = F.interpolate(inv2_mask3d.unsqueeze(0).unsqueeze(0), size=(300, 512, 512), mode='trilinear').squeeze(0).squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJNy2c0ocixe"
      },
      "source": [
        "## Optional: display results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "3GZ1rNTyWfEN",
        "outputId": "89cb561e-5205-4fe6-9d44-679daebfe92f"
      },
      "outputs": [],
      "source": [
        "#view slice of inv2, original t1map, mask,\n",
        "'''\n",
        "slice_idx = 5\n",
        "\n",
        "print(t1map_3d.size())\n",
        "\n",
        "plt.subplot(1, 5, 1)\n",
        "#display original t1map image\n",
        "plt.imshow(t1map_3d[slice_idx].squeeze(0), cmap=\"gray\")\n",
        "plt.subplot(1, 5, 2)\n",
        "#display a slice of t1map mask\n",
        "plt.imshow(t1map_mask3d[slice_idx].cpu(), cmap=\"gray\")\n",
        "plt.subplot(1, 5, 3)\n",
        "#display a slice from t1map edited with t1map mask\n",
        "plt.imshow(edited_t1map_3d_from_t1map[slice_idx].squeeze(0), cmap=\"gray\")\n",
        "plt.subplot(1, 5, 4)\n",
        "#display a slice of inv2 mask\n",
        "plt.imshow(inv2_mask3d[slice_idx].cpu(), cmap=\"gray\")\n",
        "plt.subplot(1, 5, 5)\n",
        "#display a slice from t1map edited with inv2 mask\n",
        "plt.imshow(edited_t1map_3d_from_inv2[slice_idx].squeeze(0), cmap=\"gray\")\n",
        "\n",
        "print(edited_t1map_3d_from_inv2[5, 75, 75], torch.min(edited_t1map_3d_from_inv2))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csOg-bmIluBw"
      },
      "source": [
        "## Save edited images\n",
        "Save edited t1map images back as their original DICOM files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cbb7089",
        "outputId": "25e7812c-2efd-4b62-a56d-4dd5d1a2f526"
      },
      "outputs": [],
      "source": [
        "#edit dicom file and save based on inv2 mask\n",
        "for edit_slice_idx in range(edited_t1map_3d_from_inv2.size(0)):\n",
        "    # Get the original dataset for the slice\n",
        "    original_dataset = dataset[edit_slice_idx][5] # Assuming index 5 is the t1map_info object\n",
        "\n",
        "    # Create a new dataset object with the same metadata\n",
        "    new_dataset = pydicom.Dataset()\n",
        "    for data_element in original_dataset:\n",
        "        if data_element.tag != pydicom.tag.Tag(0x7fe0, 0x0010): # Exclude the original PixelData tag\n",
        "            new_dataset[data_element.tag] = data_element\n",
        "\n",
        "    # Add the modified pixel data\n",
        "    # Ensure the data type is appropriate for DICOM (e.g., int16 or uint16)\n",
        "    # You might need to scale/cast the tensor data\n",
        "    modified_pixel_data = edited_t1map_3d_from_inv2[edit_slice_idx].squeeze(0).numpy().astype('int16') # Example cast to int16\n",
        "\n",
        "    new_dataset.PixelData = modified_pixel_data.tobytes()\n",
        "    new_dataset.Rows, new_dataset.Columns = modified_pixel_data.shape\n",
        "\n",
        "    # Add FileMetaInformation if it exists in the original dataset\n",
        "    if hasattr(original_dataset, 'file_meta'):\n",
        "        new_dataset.file_meta = original_dataset.file_meta\n",
        "\n",
        "    # Save the new DICOM file\n",
        "    new_dataset.save_as(f'./lemon_data/edited_t1map/edited_t1map_{edit_slice_idx}.dcm')\n",
        "\n",
        "print(\"DICOM files saved successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
