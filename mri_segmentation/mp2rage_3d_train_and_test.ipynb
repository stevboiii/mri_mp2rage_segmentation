{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XHcEjJDDNLQt",
      "metadata": {
        "id": "XHcEjJDDNLQt"
      },
      "source": [
        "# This is the training and test program for 3D U-net for MP2RAGE MRI images\n",
        "Training was done using the MPI-LEMON dataset of MP2RAGE images found at [MPI-LEMON MRI Download Page](https://fcon_1000.projects.nitrc.org/indi/retro/MPI_LEMON/downloads/download_MRI.html)\n",
        "\n",
        "Make sure to update and verify all of the file and folder directories\n",
        "\n",
        "Details about hardware requirements will be specified in the README\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mk_161cylAL-",
      "metadata": {
        "id": "mk_161cylAL-"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb802c2",
      "metadata": {
        "id": "5bb802c2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy.ndimage as ndi\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.morphology import closing, remove_small_objects, disk, ball\n",
        "from scipy.ndimage import binary_fill_holes, label\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1FddN33njIzr",
      "metadata": {
        "id": "1FddN33njIzr"
      },
      "source": [
        "### If necessary: Install required packages\n",
        "\n",
        "Only with Google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xa_NH65dlcKF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa_NH65dlcKF",
        "outputId": "01e7c0f9-64fd-4f21-babd-f2dbf64cd645"
      },
      "outputs": [],
      "source": [
        "#!pip install nibabel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TOu0mlWiiHTk",
      "metadata": {
        "id": "TOu0mlWiiHTk"
      },
      "source": [
        "### If necessary: mount to drive\n",
        "Only with Google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OihRMZoCQdLB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OihRMZoCQdLB",
        "outputId": "0ae399d0-8875-4e48-8016-cbb141160095"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fq-7uxi-NQFA",
      "metadata": {
        "id": "fq-7uxi-NQFA"
      },
      "source": [
        "## Creating foreground mask using percentage threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bccbf7",
      "metadata": {
        "id": "06bccbf7"
      },
      "outputs": [],
      "source": [
        "#Generate a 3D binary mask using a percentage threshold and morphological operations.\n",
        "def create_foreground_mask(volume, threshold_percentage=0.05):\n",
        "    # Threshold to remove air/background\n",
        "    mask = volume > (threshold_percentage * np.max(volume))\n",
        "\n",
        "    # Apply 3D morphological operations\n",
        "    mask = closing(mask, ball(radius=3))\n",
        "    mask = binary_fill_holes(mask)\n",
        "    mask = remove_small_objects(mask, min_size=100)\n",
        "\n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "#Save the mask as a .nii.gz file with the same affine and header as the input image.\n",
        "def save_mask(mask, reference_nii, output_path):\n",
        "    mask_nifti = nib.Nifti1Image(mask.astype(np.uint8), affine=reference_nii.affine, header=reference_nii.header)\n",
        "    nib.save(mask_nifti, output_path)\n",
        "\n",
        "def create_mask(img_data):\n",
        "    # 1) Threshold to remove air/background\n",
        "    mask0 = img_data > (0.05 * img_data.max())\n",
        "\n",
        "    mask1 = closing(mask0, ball(radius=3))\n",
        "\n",
        "    # 2) Keep only the largest connected component (the head)\n",
        "    labels, n = ndi.label(mask1)\n",
        "    sizes = ndi.sum(mask1, labels, range(1, n+1))\n",
        "    largest = np.argmax(sizes) + 1\n",
        "    whole_head_mask = (labels == largest).astype(np.uint8)\n",
        "    return whole_head_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c2fcea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83c2fcea",
        "outputId": "9fa85de6-9f68-46a2-b3fb-584e7a4cdafa"
      },
      "outputs": [],
      "source": [
        "#generate masks for every inv2 volume\n",
        "def process_all_nii(input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    nii_files = [f for f in os.listdir(input_folder) if f.endswith(\".nii.gz\")]\n",
        "\n",
        "    for f in tqdm(nii_files, desc=\"Processing subjects\"):\n",
        "        input_path = os.path.join(input_folder, f)\n",
        "        output_path = os.path.join(output_folder, f.replace(\".nii.gz\", \"_mask.nii.gz\"))\n",
        "\n",
        "        nii = nib.load(input_path)\n",
        "        vol = nii.get_fdata()\n",
        "\n",
        "        mask = create_mask(vol)\n",
        "        save_mask(mask, nii, output_path)\n",
        "\n",
        "    print(f\"\\nDone! {len(nii_files)} mask(s) saved to: {output_folder}\")\n",
        "\n",
        "input_folder = \"./lemon_data/inv2_volumes\"\n",
        "output_folder = \"./lemon_data/inv2_masks\"\n",
        "\n",
        "process_all_nii(input_folder, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vOvbliNuOyBG",
      "metadata": {
        "id": "vOvbliNuOyBG"
      },
      "source": [
        "## Create dataset of all nii.gz files for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efa7948",
      "metadata": {
        "id": "7efa7948"
      },
      "outputs": [],
      "source": [
        "#dataset of nii.gz files\n",
        "class NiftiDataset3D(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.nii') or f.endswith('.nii.gz')])\n",
        "        self.mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.nii') or f.endswith('.nii.gz')])\n",
        "        self.transform = transform   #none for now\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = nib.load(self.image_paths[idx]).get_fdata()\n",
        "        mask = nib.load(self.mask_paths[idx]).get_fdata()\n",
        "\n",
        "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
        "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0) # Add channel dimension\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            img, mask = self.transform(img, mask)\n",
        "\n",
        "        return img, mask\n",
        "#image and mask\n",
        "inv2_dir = './lemon_data/inv2_volumes'  # Folder of .nii.gz volumes\n",
        "mask_dir = './lemon_data/inv2_masks'    # Corresponding masks\n",
        "\n",
        "#create inv2 dataset\n",
        "inv2_dataset = NiftiDataset3D(inv2_dir, mask_dir)\n",
        "inv2_loader = DataLoader(inv2_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tzguuZZvPVOf",
      "metadata": {
        "id": "tzguuZZvPVOf"
      },
      "source": [
        "## Create 3d U-net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1616e8a",
      "metadata": {
        "id": "f1616e8a"
      },
      "outputs": [],
      "source": [
        "#convolution block for a u-net\n",
        "class ConvBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "#3D U-net class\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock3D(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.enc2 = ConvBlock3D(32, 64)\n",
        "        self.pool2 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.bottleneck = ConvBlock3D(64, 128)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
        "        self.dec2 = ConvBlock3D(128, 64)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
        "        self.dec1 = ConvBlock3D(64, 32)\n",
        "\n",
        "        self.final = nn.Conv3d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        b = self.bottleneck(self.pool2(e2))\n",
        "\n",
        "        d2 = self.upconv2(b)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        return self.final(d1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bd6c90",
      "metadata": {},
      "source": [
        "Dice loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zTSDbT5KPydd",
      "metadata": {
        "id": "zTSDbT5KPydd"
      },
      "outputs": [],
      "source": [
        "#loss function: dice loss\n",
        "def dice_loss(pred, target, smooth=1e-5):\n",
        "    pred = torch.sigmoid(pred)  # if using raw logits\n",
        "    pred_flat = pred.contiguous().view(-1)\n",
        "    target_flat = target.contiguous().view(-1).float()\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    return 1 - (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5a11c1",
      "metadata": {},
      "source": [
        "Two models: One trained with inv2 and one with t1map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zcjTMucyP7S9",
      "metadata": {
        "id": "zcjTMucyP7S9"
      },
      "outputs": [],
      "source": [
        "#use cuda or cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#Inv2 model\n",
        "inv2model = UNet3D().to(device)\n",
        "#Adam optimizer\n",
        "optimizer = torch.optim.Adam(inv2model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zA4JDKutPcQS",
      "metadata": {
        "id": "zA4JDKutPcQS"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N09qM8BLP-Ni",
      "metadata": {
        "id": "N09qM8BLP-Ni"
      },
      "outputs": [],
      "source": [
        "#training loop\n",
        "def train(epochs, model, loader, accumulate_grad_batches=4):\n",
        "    model.train()\n",
        "    scaler = GradScaler()  # Initialize GradScaler for mixed precision\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for i, (imgs, masks) in enumerate(loader):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "\n",
        "            with autocast():  # Autocast for mixed precision\n",
        "                outputs = model(imgs)\n",
        "                loss = dice_loss(outputs, masks)\n",
        "                loss = loss / accumulate_grad_batches\n",
        "\n",
        "            scaler.scale(loss).backward()  # Scale the loss and call backward()\n",
        "\n",
        "            if (i + 1) % accumulate_grad_batches == 0:\n",
        "                scaler.step(optimizer)  # Optimizer step with scaler\n",
        "                scaler.update()  # Update the scaler\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulate_grad_batches\n",
        "\n",
        "        if (i + 1) % accumulate_grad_batches != 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_HVa1akKT8oE",
      "metadata": {
        "id": "_HVa1akKT8oE"
      },
      "source": [
        "### training\n",
        "Do not load parameters first before your first training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qn1SxMAsS_io",
      "metadata": {
        "id": "qn1SxMAsS_io"
      },
      "outputs": [],
      "source": [
        "#load parameters\n",
        "inv2model.load_state_dict(torch.load('./3d_segmentation_inv2.pth', map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de19UAWyTxKa",
      "metadata": {
        "id": "de19UAWyTxKa"
      },
      "outputs": [],
      "source": [
        "#train and save parameters\n",
        "train(5, inv2model, inv2_loader)\n",
        "torch.save(inv2model.state_dict(), './3d_segmentation.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tjXLXs4uUDcU",
      "metadata": {
        "id": "tjXLXs4uUDcU"
      },
      "source": [
        "## Predicting mask to test\n",
        "functions to predict and edit volume based on mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h4HhpKppDSVR",
      "metadata": {
        "id": "h4HhpKppDSVR"
      },
      "outputs": [],
      "source": [
        "#function to predict mask using data\n",
        "def predict(model, image_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Reshape tensor to [Batch, Channels, Depth, Height, Width]\n",
        "        # Assuming image_tensor is [D, H, W] or [1, D, H, W]\n",
        "        if image_tensor.ndim == 3:\n",
        "            image_tensor = image_tensor.unsqueeze(0).unsqueeze(0) # [1, 1, D, H, W]\n",
        "        elif image_tensor.ndim == 4:\n",
        "            image_tensor = image_tensor.unsqueeze(0) # [1, 1, D, H, W]\n",
        "\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        output = model(image_tensor)\n",
        "        return output.cpu()\n",
        "\n",
        "#edit an image based on mask\n",
        "def maskEditing(image, mask, threshold=-0.5):\n",
        "    new_img = torch.tensor(image.copy())  # Create a new tensor from a NumPy copy\n",
        "    if not torch.is_tensor(mask):\n",
        "        mask = torch.tensor(mask)\n",
        "\n",
        "    # Ensure mask has the same spatial dimensions as new_img\n",
        "    # Assuming mask is [1, 1, D, H, W] and new_img is [D, H, W]\n",
        "    if mask.ndim == 5:\n",
        "        mask = mask.squeeze(0).squeeze(0) # [D, H, W]\n",
        "\n",
        "    for l in range(mask.size(0)): # Iterate over depth\n",
        "        for w in range(mask.size(1)): # Iterate over height\n",
        "            for h in range(mask.size(2)): # Iterate over width\n",
        "                 if mask[l, w, h] < threshold:\n",
        "                    new_img[l, w, h] = -1  #set background voxels to -1\n",
        "\n",
        "    return new_img  # return as numpy array to keep format consistent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JdbqLdo0GdGW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdbqLdo0GdGW",
        "outputId": "46e6ab0f-7869-4eb5-a2da-8317d668d8bb"
      },
      "outputs": [],
      "source": [
        "slice_idx = 125  #index of one layer of head\n",
        "\n",
        "# Load the full 3D volume and mask\n",
        "full_inv2_volume = inv2_dataset[12][0]  # Shape: [1, D, H, W]\n",
        "full_mask_volume = inv2_dataset[12][1]  # Shape: [1, D, H, W]\n",
        "\n",
        "#load corresponding t1map image\n",
        "t1map = torch.from_numpy(nib.load(\"/content/drive/My Drive/lemon_data/t1map_volumes/sub-032394_ses-01_acq-mp2rage_T1map.nii.gz\").get_fdata()).float()\n",
        "t1map_slice = t1map[slice_idx, :, :] #get slice of t1map image\n",
        "print(type(t1map), type(full_inv2_volume), t1map.size())\n",
        "\n",
        "t1map_mask = predict(inv2model, t1map) #generate mask based on t1map\n",
        "\n",
        "binaryMask = (t1map_mask > 0).float() #create binary mask\n",
        "\n",
        "#edit t1map image based on mask\n",
        "edited_t1map = maskEditing(t1map.numpy(), binaryMask, threshold=0.5) # Pass numpy array to maskEditing\n",
        "edited_t1map_slice = edited_t1map[slice_idx, :, :]\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(t1map_mask.squeeze().detach().numpy()[slice_idx, :, :], cmap=\"gray\") #display ground truth mask\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(binaryMask.squeeze().detach().numpy()[slice_idx, :, :], cmap=\"gray\") #display generated mask\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(t1map_slice, cmap=\"gray\") #display original t1map image\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(edited_t1map_slice, cmap=\"gray\") #display edited t1map image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
